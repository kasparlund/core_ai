{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import *\n",
    "\n",
    "#let me be on my mac\n",
    "def cuda(self, device=None, non_blocking=False) : return self\n",
    "torch.Tensor.cuda = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.callbacks import *\n",
    "from lib.model import *\n",
    "from lib.data import *\n",
    "from lib.optimizers import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create basemodel on imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGEWOOF_160)\n",
    "#path = untar_data(datasets.URLs.IMAGENETTE_160)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "bs   = 64\n",
    "\n",
    "tfms     = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), PilRandomFlip(), to_byte_tensor, to_float_tensor]\n",
    "val_tfms = [make_rgb, CenterCrop(size), to_byte_tensor, to_float_tensor]\n",
    "files    = ImageList.from_files(path, tfms=tfms)\n",
    "\n",
    "sd       = SplitData.split_by_func(files, partial(grandparent_splitter, valid_name='val'))\n",
    "data     = label_train_valid_data(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "data.valid.x.tfms  = val_tfms\n",
    "\n",
    "imagenette_features = max(data.train.y)+1\n",
    "print(f\"number of training, validation images: {len(data.train)},  {len(data.valid)}\")\n",
    "print(f\"imagenette_features:{imagenette_features}\")\n",
    "\n",
    "train_dl,valid_dl = ( DataLoader(data.train, batch_size=bs,   num_workers=4, shuffle=True),\n",
    "                      DataLoader(data.valid, batch_size=bs*2, num_workers=4))\n",
    "databunch = DataBunch(train_dl, valid_dl, c_in=3, c_out=imagenette_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_sizes = [64,64,128,256]\n",
    "layer = partial( conv_layer, stride=2, bn=True, zero_bn=False, act=partial(torch.nn.ReLU,inplace=True) )\n",
    "model = get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer)\n",
    "init_cnn( model )\n",
    "\n",
    "cbfs_base = [TrainableModelCallback, TrainEvalCallback, OptimizerCallback, \n",
    "#        partial(ParamScheduler, 'lr', sched),\n",
    "        partial(BatchTransformXCallback, norm_imagenette),\n",
    "#        partial(MixUp,Î±=0.4),\n",
    "        \n",
    "        #CudaCallback,\n",
    "        ProgressCallback,\n",
    "       ]\n",
    "cbfs = cbfs_base.copy() + [Recorder, partial(AvgStatsCallback,[accuracy])]\n",
    "cbfs_lr_Finder = cbfs_base.copy() + [LR_Finder]\n",
    "\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grads_summary(model)\n",
    "#xb,_ = getFirstbatch( model, databunch, partial(BatchTransformXCallback, tfm = norm_imagenette))\n",
    "#model_summary(model, xb, only_leaves=True, print_mod=False)\n",
    "print(f\"\\nmodel hierarchy:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner( model, databunch, loss_func=LabelSmoothingCrossEntropy())\n",
    "%time learn.fit(5, opt=Adam(sched,max_lr=3e-4, moms=(0.85,0.95), max_wd = 1e-4), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(path, learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains Pets dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = datasets.untar_data(datasets.URLs.PETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pets.iterdir())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_path = pets/'images'\n",
    "list(pets_path.iterdir())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def random_splitter(fn, p_valid): return random.random() < p_valid\n",
    "def pet_labeler(fn): return re.findall(r'^(.*)_\\d+.jpg$', fn.name)[0]\n",
    "\n",
    "files = ImageList.from_files(pets_path, tfms=tfms)\n",
    "sd    = SplitData.split_by_func(files, partial(random_splitter, p_valid=0.1))\n",
    "\n",
    "proc  = CategoryProcessor()\n",
    "data  = label_train_valid_data(sd, pet_labeler, proc_y=proc)\n",
    "data.valid.x.tfms = val_tfms\n",
    "\n",
    "pets_features     = len(proc.vocab)\n",
    "print(f\"number of training, validation images: {len(data.train)},  {len(data.valid)}\")\n",
    "print(f\"pets_features:{pets_features}\")\n",
    "\n",
    "train_dl,valid_dl = ( DataLoader(data.train, batch_size=bs,   num_workers=4, shuffle=True),\n",
    "                      DataLoader(data.valid, batch_size=bs*2, num_workers=4))\n",
    "databunch = DataBunch(train_dl, valid_dl, c_in=3, c_out=pets_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"categories:\\n{ ', '.join(proc.vocab) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with from scratch\n",
    "model = get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer)\n",
    "init_cnn( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xb,_ = getFirstbatch( learn.model, databunch, partial(BatchTransformXCallback, tfm = norm_imagenette))\n",
    "#model_summary(model, xb, only_leaves=True, print_mod=False)\n",
    "print(f\"model hierarchy:\\n{model}\")\n",
    "model_grads_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner( model, databunch, loss_func=LabelSmoothingCrossEntropy() )\n",
    "cbfs  = cbfs_base.copy() + [Recorder, partial(AvgStatsCallback,[accuracy])]\n",
    "%time learn.fit(2, opt=Adam(sched,max_lr=3e-4, moms=(0.85,0.95), max_wd = 1e-4), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use pretrained imagewoff model for training with gradual unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"path to pretrained model:{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained on imagewoof\n",
    "model = get_cnn_model(layers_sizes, 3, imagenette_features, layer)\n",
    "load_model(path, model)\n",
    "\n",
    "print(f\"model hierarchy:\\n{model}\")\n",
    "model_grads_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = adapt_model(model, databunch, norm=norm_imagenette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freeze(model)\n",
    "model_grads_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs  = cbfs_base.copy() + [Recorder, partial(AvgStatsCallback,[accuracy])]\n",
    "learn = Learner( model, databunch, loss_func=LabelSmoothingCrossEntropy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1, opt=Adam(sched,max_lr=1e-2, moms=(0.85,0.95), max_wd = 1e-4), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch\ttrain_loss\ttrain_accuracy\tvalid_loss\tvalid_accuracy\ttime\n",
    "0\t3.388485\t0.099083\t3.246178\t0.146143\t00:49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unfreeze(learn.model)\n",
    "model_grads_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, opt=Adam(sched,max_lr=5e-5, moms=(0.85,0.95), max_wd = 1e-6), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch\ttrain_loss\ttrain_accuracy\tvalid_loss\tvalid_accuracy\ttime\n",
    "0\t3.198439\t0.156367\t3.217241\t0.162382\t01:07"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
