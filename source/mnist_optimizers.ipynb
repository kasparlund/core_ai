{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notebook minist and different optimizers\n",
    "\n",
    "the basis is kaiminig optimization and batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import *\n",
    "\n",
    "#let me be on my mac\n",
    "def cuda(self, device=None, non_blocking=False) : return self\n",
    "torch.Tensor.cuda = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.callbacks import *\n",
    "from lib.model import *\n",
    "from lib.data import *\n",
    "from lib.optimizers import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_mnist_data(Path(\"/Users/kasparlund/.fastai/data/mnist.pkl.gz\"))\n",
    "x_train,x_valid   = normalize_to(x_train,x_valid)\n",
    "\n",
    "x_train.mean(),x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh,bs              = 50,512\n",
    "train_ds,valid_ds  = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "train_dl, valid_dl = ( DataLoader( train_ds, batch_size=bs, shuffle=True), \n",
    "                        DataLoader(valid_ds,  batch_size=bs*2) )\n",
    "data               = DataBunch( train_dl, valid_dl, c_in=1, c_out=y_train.max().item()+1 )\n",
    "print(len(train_dl)), print(len(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "mnist_view   = view_tfm(1,28,28)\n",
    "layers_sizes = [8,16,32,32]\n",
    "loss_func    = F.cross_entropy\n",
    "sched        = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) \n",
    "\n",
    "cbfs         = [TrainableModelCallback, TrainEvalCallback, OptimizerCallback, \n",
    "#                partial(ParamScheduler, 'lr', sched),\n",
    "                partial(BatchTransformXCallback, tfm = mnist_view), \n",
    "#                partial(MixUp,Î±=0.2),\n",
    "#                LR_Finder,\n",
    "                Recorder, \n",
    "                partial(AvgStatsCallback,[accuracy]),\n",
    "                ProgressCallback\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnets + steppers=[sgd_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([512, 8, 14, 14])\n",
      " torch.Size([512, 16, 7, 7])\n",
      " torch.Size([512, 32, 4, 4])\n",
      " torch.Size([512, 32, 2, 2])\n",
      " torch.Size([512, 32, 1, 1])\n",
      " torch.Size([512, 32])\n",
      " torch.Size([512, 10])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.801429</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.217254</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 4.68 s, total: 30.4 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = SGD(sched,max_lr=0.8)#0.5)\n",
    "#loss_func=LabelSmoothingCrossEntropy()\n",
    "loss_func=F.cross_entropy\n",
    "\n",
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "\n",
    "#model  = partial(xresnet18, c_in=data.c_in, c_out=data.c_out)()\n",
    "learn = Learner( model, data, loss_func=loss_func, opt=opt,cb_funcs=cbfs)\n",
    "\n",
    "xb,_ = getFirstbatch( learn.model, data, partial(BatchTransformXCallback, tfm = mnist_view))\n",
    "#print(f\"xb.shape:{xb.shape}\")\n",
    "#print(learn.model)\n",
    "model_summary(learn.model, xb, print_mod=False)\n",
    "%time learn.fit(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with renet18\n",
    "epoch\ttrain_loss\ttrain_accuracy\tvalid_loss\tvalid_accuracy\ttime\n",
    "0\t0.284343\t0.915040\t0.072470\t0.978900\t03:33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steppers=[sgd_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "opt = SGD(sched,max_lr=0.8)#0.5)\n",
    "#loss_func=LabelSmoothingCrossEntropy()\n",
    "loss_func=F.cross_entropy\n",
    "learn = Learner( model, data, loss_func=loss_func, opt=opt,cb_funcs=cbfs)\n",
    "\n",
    "%time learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steppers=[weight_decay, sgd_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "opt = SGD(sched,max_lr=0.5, max_wd=0.01)\n",
    "#print(opt.hypers[0]['lr']), print(opt.hypers[0]['wd'])\n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt,cb_funcs=cbfs)\n",
    "%time learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## momentum: steppers=[momentum_step,weight_decay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )        \n",
    "opt   = SGD_Momentum(sched,max_lr=1.0, moms=(0.85,0.95), max_wd=1e-3 )\n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt, cb_funcs=cbfs)\n",
    "%time learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=0),plt.show()\n",
    "\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam: steppers=[adam_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "opt   = Adam(sched,max_lr=0.05, moms=(0.85,0.95), max_wd = 1e-3)\n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt, cb_funcs=cbfs)\n",
    "%time learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=10),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB: steppers=[lamb_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then super easy to implement a new optimizer. This is LAMB from a [very recent paper](https://arxiv.org/pdf/1904.00962.pdf):\n",
    "\n",
    "$\\begin{align}\n",
    "g_{t}^{l} &= \\nabla L(w_{t-1}^{l}, x_{t}) \\\\\n",
    "m_{t}^{l} &= \\beta_{1} m_{t-1}^{l} + (1-\\beta_{1}) g_{t}^{l} \\\\\n",
    "v_{t}^{l} &= \\beta_{2} v_{t-1}^{l} + (1-\\beta_{2}) g_{t}^{l} \\odot g_{t}^{l} \\\\\n",
    "m_{t}^{l} &= m_{t}^{l} / (1 - \\beta_{1}^{t}) \\\\\n",
    "v_{t}^{l} &= v_{t}^{l} / (1 - \\beta_{2}^{t}) \\\\\n",
    "r_{1} &= \\|w_{t-1}^{l}\\|_{2} \\\\\n",
    "s_{t}^{l} &= \\frac{m_{t}^{l}}{\\sqrt{v_{t}^{l} + \\epsilon}} + \\lambda w_{t-1}^{l} \\\\ \n",
    "r_{2} &= \\| s_{t}^{l} \\|_{2} \\\\\n",
    "\\eta^{l} &= \\eta * r_{1}/r_{2} \\\\ \n",
    "w_{t}^{l} &= w_{t}^{l-1} - \\eta_{l} * s_{t}^{l} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )            \n",
    "opt   = LAMB(sched,max_lr=0.02, moms=(0.85,0.95), max_wd = 1e-4)     \n",
    "#opt   = LAMB(sched,max_lr=0.005, moms=(0.85,0.95), max_wd = 1e-4)     #with mixup\n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt, cb_funcs=cbfs)\n",
    "%time learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist with adam and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "opt   = Adam(sched,max_lr=0.05, moms=(0.85,0.95), max_wd = 1e-3)\n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt, cb_funcs=cbfs)\n",
    "with Hooks(model, append_stats) as hooks: \n",
    "    learn.fit(1)\n",
    "    plot_layer_stats( hooks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mnist with LAMB and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(layers_sizes, data.c_in, data.c_out, partial(conv_layer, stride=2) )\n",
    "init_cnn( model )\n",
    "opt   = LAMB(sched,max_lr=0.008, moms=(0.85,0.95), max_wd = 1e-6)     \n",
    "learn = Learner( model, data, loss_func=F.cross_entropy, opt=opt, cb_funcs=cbfs)\n",
    "with Hooks(model, append_stats) as hooks: \n",
    "    learn.fit(1)\n",
    "    plot_layer_stats( hooks )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
